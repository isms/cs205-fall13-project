<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="">
    <meta name="author" content="">
    <link rel="shortcut icon" href="http://pages.github.com/favicon.ico">

    <title>CS 205 Final Project</title>

    <!-- Bootstrap core CSS -->
    <link href="//netdna.bootstrapcdn.com/bootswatch/3.0.2/journal/bootstrap.min.css" rel="stylesheet">

    <!-- Custom styles for this template -->
    <link href="http://getbootstrap.com/examples/jumbotron-narrow/jumbotron-narrow.css" rel="stylesheet">

    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {inlineMath: [['$','$']]}
      });
    </script>
    <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
    </script>
  </head>

  <body>

    <div class="container">
      <div class="header">
        <ul class="nav nav-pills pull-right">
          <li><a href="index.html">Home</a></li>
          <li class="active"><a href="results.html">Results</a></li>
          <li><a href="https://github.com/isms/cs205-fall13-project" target="_blank"><img src="img/GitHub-Mark-32px.png" height="18"/> Code</a></li>
        </ul>
        <h3 class="text-muted">CS 205 Project</h3>
      </div>

      <div class="container">
        <h2>Parallelizing an ILP Assignment Problem</h2>
        <h3 class="text-muted">Results</h3>
      </div>

      <div class="row marketing">
        <div class="col-md-12">
          <h4>Setup</h4>

          <p>We modeled the underlying ILP with <code><a href="https://code.google.com/p/pulp-or/">pulp</a></code> using the <code><a href="http://www.gnu.org/software/glpk/">GNU GLPK</a></code> as the back-end solver. We used <code><a href="http://mpi4py.scipy.org/">mpi4py</a></code> to parallelize solution finding.</p>

          <h5>Notation</h5>

          <p>Let $m$ denote the number of students and $n$ denote the number of classes. Let $x_{ij}$ be a binary variable with $x_{ij} = 1$ indicating that student $i$ is assigned to class $j$, for $i = 1, ..., m$, $j = 1, ..., n$.</p>

          <h5>Objective function</h5>

          <p>Our true goal in the ILP is not to find the "best" ILP solution &mdash; we actually evaluate solution fitness in the non-linear step &mdash; instead we are simply trying to generate as many feasible solutions as possible subject to our hard constraints. We create an arbitrary objective function:</p>

              <p style="text-align:center">
                 $$\operatorname{min} Z = \sum_{j=1}^{n} \sum_{i=1}^{m} x_{ij} \cdot i \cdot j$$
              <p>

          <p>This objective function can be used to get the solver to generate a different feasible solutions. Specifically, we can take the objective function minimum found in the last iteration &mdash; call it $z_{t-1}$ &mdash; and add a new constraint for the current iteration that the objective function $Z > z_{t-1}$. For our data set, our solver found a minimum value for the objective function of 9,227 and a maximum value of 11,534. Thus, the upper bound on the number of solutions we could generate is 1,608. However, the solver is not always able to find a solution at each integer within this range. This is especially true near the endpoints of the domain (we were able to find a total of 1409 solutions for our dataset). Thus, we face two issues when trying to parallelize this process: first that the ILP solver takes different amounts of time for each solution (ranging from about 1 second to 1 minute), thus creating imbalanced workloads, and secondly that we don't know ahead of time how many times the solver will be called within a given range.</p>

          <p><small><strong>Note:</strong> some solvers are able to generate additional feasible solutions from the same LP model, but many are not. Though we opted to use <code>GLPK</code> for benchmarking purposes, we wanted to avoid making assumptions about the solver. For that reason, we chose this method of an artificial objective function with iterative constraints in order to generate multiple feasible solutions.</small></p>

          <h5>Constraints</h5>

          <ol>
            <li> All decision variables must be binary.

                <p style="text-align:center">
                   $$x_{ij} \in \{0, 1\}, \; \forall \; i \in \{1, ..., m\} \text{; } j \in \{1, ..., n\}$$
                </p></li>

            <li>Every student must be assigned to a class.</p>

                <p style="text-align:center">
                   $$\sum_{j=1}^{n} x_{ij} = 1, \; i \in \{1, ..., m\}$$
                </p></li>

            <li>A maximum class size $k$ must be respected.</p>

                <p style="text-align:center">
                   $$\sum_{i=1}^{m} x_{ij} \leq k, \; j \in \{1, ..., n\}$$
                </p></li>

            <li>Every student must be in a class with one of his or her designated friends (in a friend list $L_i$ of friends for student $i$).</p>

                <p style="text-align:center">
                   $$x_{ij} \leq \sum_{s \in L_i} x_{sj}, \; i \in \{1, ..., m\} \text{; } j \in \{1, ..., n\}$$
                </p></li>

            <li>Certain other constraints involving the male/female ratio in a class, or students who should be kept apart.</li>
          </ol>

          <h4>Basic implementation</h4>

          <p>The simplest implementation using MPI is to start each process off with a roughly equal portion of the integers in between the absolute min and max of the artificial objective function, and let them simply work through their respective assignments without communicating further until work collection by the root process.</p>

          <p>The obvious problem with this method &mdash; we'll call it the "naive" implementation &mdash; is that some processors may finish long before the others and waste time doing nothing. Nonetheless, there is large initial win simply by dividing up the work among many processors. We seek to improve on this initial improvement by being smarter about keeping all processes employed.</p>

          <div class="panel panel-default">
            <div class="panel-body">
              <img src="img/result-naive.png" width="600" />
            </div>
            <div class="panel-footer">Results from the naive implementation.</div>
          </div>

          <h4>Distributed load balancing</h4>

          <p>One way to split up work in MPI is to start each process off with a roughly equal portion of the domain (as in the naive implementation), but let them communicate directly with one another if they run out of work. In order to do this, each process runs with a "work" thread that solves the ILPs and a communication thread that sends out requests for work when the work thread finishes and also "listens" for requests from other processors. When a processor receives a request for work, if it has more than a minimum threshold of work left within its given range, it hands off half of the work to the requesting processor. If it does not have work, then it sends a rejection and the processor requesting work will then move on to request work from the next processor. If the processor receives enough rejections, then it simply gives up and terminates (although it's communication thread will stay alive until the all processors are done in order to send out rejections when it receives work).</p>

          <p>The major problem with this method is that having each processor running multiple threads slows down the speed at which they do work. To alleviate this issue, we have the communication thread pause for two seconds between each iteration of its loop. This works well for a low number of processors, but does not scale well since the communication overhead increases with the number of processors, so the communication thread does a significant amount of work despite the pauses (we experimented with various pause lengths, but increasing the pause length too much slows down the average response time when processors request work. We found 2 seconds to be a good balance). Additionally, even without any pause, as the number of processors increases, so too does the response time since each processor must check for message from all other processors.</p>

          <div class="panel panel-default">
            <div class="panel-body">
              <img src="img/distributed-load-balancing.png" width="600" />
            </div>
            <div class="panel-footer">Distributed load balancing.</div>
          </div>

          <div class="panel panel-default">
            <div class="panel-body">
              <img src="img/result-distributed.png" width="600" />
            </div>
            <div class="panel-footer">Results from the distributed load balancing implementation.</div>
          </div>

          <p>As the results above show, with fewer processors, this was a relatively fast and efficient method. However, the communication overhead increases quickly as the number of processes is increased, which actually causes the process to slow down when too many processors are used. Another reason that it doesn't scale well for this problem is that we have a limited domain. When using 256 processors, each processor gets a "chunk" of about 6 integers from the range between 9,927 and 11,534. With such a small amount of work to be completed and a large amount of communication overhead slowing down each processor, it is more efficient to simply divide the work up "naively" and process it without any communication. Because of these scaling issues, we decided to implement the master/slave implementations discussed below.</p>

          <div class="panel panel-default">
            <div class="panel-body">
              <table class="table table-striped table-condensed">
                <thead>
                  <tr>
                    <th>processors</th>
                    <th>load balancing</th>
                    <th>time (s)</th>
                    <th>speedup</th>
                    <th>efficiency</th>
                  </tr>
                </thead>
                <tbody>
                  <tr class="warning">
                    <td>   1</td>
                    <td> ---</td>
                    <td> 13500.3095</td>
                    <td> ---</td>
                    <td> ---</td>
                  </tr>
                  <tr>
                    <td>   2</td>
                    <td>  No</td>
                    <td> 7150.7547</td>
                    <td>  1.89</td>
                    <td> 0.94</td>
                  </tr>
                  <tr>
                    <td>   4</td>
                    <td>  No</td>
                    <td> 3910.5397</td>
                    <td>   3.45</td>
                    <td> 0.86</td>
                  </tr>
                  <tr>
                    <td>   8</td>
                    <td>  No</td>
                    <td> 2411.4662</td>
                    <td>   5.60</td>
                    <td> 0.70</td>
                  </tr>
                  <tr>
                    <td>  16</td>
                    <td>  No</td>
                    <td> 1096.7792</td>
                    <td>  12.31</td>
                    <td> 0.77</td>
                  </tr>
                  <tr>
                    <td>  32</td>
                    <td>  No</td>
                    <td>  650.0861</td>
                    <td>  20.76</td>
                    <td> 0.65</td>
                  </tr>
                  <tr>
                    <td>  64</td>
                    <td>  No</td>
                    <td>  381.0861</td>
                    <td>  35.43</td>
                    <td> 0.55</td>
                  </tr>
                  <tr>
                    <td> 128</td>
                    <td>  No</td>
                    <td>  221.8088</td>
                    <td>  60.86</td>
                    <td> 0.48</td>
                  </tr>
                  <tr>
                    <td> 256</td>
                    <td>  No</td>
                    <td>  143.8961</td>
                    <td>  93.82</td>
                    <td> 0.37</td>
                  </tr>
                  <tr>
                    <td>   2</td>
                    <td> Yes</td>
                    <td> 6799.1015</td>
                    <td> 1.99</td>
                    <td> 0.99</td>
                  </tr>
                  <tr>
                    <td>   4</td>
                    <td> Yes</td>
                    <td> 3641.5851</td>
                    <td> 3.71</td>
                    <td> 0.93</td>
                  </tr>
                  <tr>
                    <td>   8</td>
                    <td> Yes</td>
                    <td> 1787.092</td>
                    <td>  7.55 </td>
                    <td> 0.94</td>
                  </tr>
                  <tr>
                    <td>  16</td>
                    <td> Yes</td>
                    <td>  1063.6516</td>
                    <td> 12.69</td>
                    <td> 0.79</td>
                  </tr>
                  <tr>
                    <td>  32</td>
                    <td> Yes</td>
                    <td> 555.0021</td>
                    <td> 24.32 </td>
                    <td> 0.76</td>
                  </tr>
                  <tr>
                    <td> 64</td>
                    <td> Yes</td>
                    <td> 401.2980</td>
                    <td> 33.6416</td>
                    <td> 0.49</td>
                  </tr>
                  <tr>
                    <td> 128</td>
                    <td> Yes</td>
                    <td> 217.4343</td>
                    <td> 62.09</td>
                    <td> 0.49</td>
                  </tr>
                  <tr>
                    <td> 256</td>
                    <td> Yes</td>
                    <td> 542.4713</td>
                    <td> 24.89</td>
                    <td> 0.10</td>
                  </tr>
                </tbody>
              </table>
            </div>
            <div class="panel-footer">Table of results for basic implementation and distributed load balancing.</div>
          </div>

          <h4>Master/slave model</h4>

          <p>In the first iteration of master/slave, each process initially gets an even chunk of the integers in between the absolute min and max of the artificial objective function. Like the naive implementation, we do not yet do any load balancing. Additionally, we have one fewer process than the naive version because the master does not yet do any work. Although it seems probable that load balancing will have a net positive impact, we first would like to observe performance without doing so &mdash; this way we can quantify the benefits of load balancing, and see if the added overhead is worthwhile.</p>

          <div class="panel panel-default">
            <div class="panel-body">
              <img src="img/mpi-master-slave.png" width="600" />
            </div>
            <div class="panel-footer">Master/slave using even work assignments for all processes.</div>
          </div>

          <p>Like the naive implementation, this method resulted in excellent speedups over the serial version &mdash; the following graph shows both speedup and efficiency with different numbers of processors.</p>

          <div class="panel panel-default">
            <div class="panel-body">
              <img src="img/result-no-split.png" width="600" />
            </div>
            <div class="panel-footer">Results from solving ILP with simple master/slave.</div>
          </div>

          <h4>Master/slave model with load balancing</h4>

          <p>To be even more efficient, we don't want any of our slave processes to finish early (which they often do because feasible solutions are not evenly distributed through the space of integers being divided up). In our next iteration, we implemented a scheme where the master would attempt to redistribute work from the slowest process to any process that had finished all of its initial work assignment.</p>

          <div class="panel panel-default">
            <div class="panel-body">
              <img src="img/mpi-splitting-work.png" width="600" />
            </div>
            <div class="panel-footer">Master/slave with load balancing to keep all processes employed.</div>
          </div>

          Load balancing was even faster (and slightly more efficient) than the previous implementation.

          <div class="panel panel-default">
            <div class="panel-body">
              <img src="img/result-split.png" width="600" />
            </div>
            <div class="panel-footer">Results from solving ILP with load balancing master/slave.</div>
          </div>

          <div class="panel panel-default">
            <div class="panel-body">
              <table class="table table-striped table-condensed">
                <thead>
                  <tr>
                    <th>processors</th>
                    <th>load balancing</th>
                    <th>time (s)</th>
                    <th>speedup</th>
                    <th>efficiency</th>
                  </tr>
                </thead>
                <tbody>
                  <tr class="warning">
                    <td>   1</td>
                    <td> ---</td>
                    <td> 13500.3095</td>
                    <td> ---</td>
                    <td> ---</td>
                  </tr>
                  <tr>
                    <td>   4</td>
                    <td>  No</td>
                    <td> 4922.5975</td>
                    <td>   2.74</td>
                    <td> 0.69</td>
                  </tr>
                  <tr>
                    <td>   8</td>
                    <td>  No</td>
                    <td> 2212.6460</td>
                    <td>   6.10</td>
                    <td> 0.76</td>
                  </tr>
                  <tr>
                    <td>  16</td>
                    <td>  No</td>
                    <td> 1148.0969</td>
                    <td>  11.76</td>
                    <td> 0.73</td>
                  </tr>
                  <tr>
                    <td>  32</td>
                    <td>  No</td>
                    <td>  581.3491</td>
                    <td>  23.22</td>
                    <td> 0.73</td>
                  </tr>
                  <tr>
                    <td>  64</td>
                    <td>  No</td>
                    <td>  325.5046</td>
                    <td>  41.48</td>
                    <td> 0.69</td>
                  </tr>
                  <tr>
                    <td> 128</td>
                    <td>  No</td>
                    <td>  186.2928</td>
                    <td>  72.49</td>
                    <td> 0.57</td>
                  </tr>
                  <tr>
                    <td> 256</td>
                    <td>  No</td>
                    <td>  161.1719</td>
                    <td>  83.76</td>
                    <td> 0.33</td>
                  </tr>
                  <tr>
                    <td>   4</td>
                    <td> Yes</td>
                    <td> 4538.3395</td>
                    <td>   2.97</td>
                    <td> 0.74</td>
                  </tr>
                  <tr>
                    <td>   8</td>
                    <td> Yes</td>
                    <td> 1965.5124</td>
                    <td>   6.87</td>
                    <td> 0.86</td>
                  </tr>
                  <tr>
                    <td>  16</td>
                    <td> Yes</td>
                    <td>  961.8448</td>
                    <td>  14.04</td>
                    <td> 0.88</td>
                  </tr>
                  <tr>
                    <td>  32</td>
                    <td> Yes</td>
                    <td>  481.9911</td>
                    <td>  28.01</td>
                    <td> 0.88</td>
                  </tr>
                  <tr>
                    <td>  64</td>
                    <td> Yes</td>
                    <td>  300.3186</td>
                    <td>  44.95</td>
                    <td> 0.70</td>
                  </tr>
                  <tr>
                    <td> 128</td>
                    <td> Yes</td>
                    <td>  179.7172</td>
                    <td>  75.12</td>
                    <td> 0.59</td>
                  </tr>
                  <tr>
                    <td> 256</td>
                    <td> Yes</td>
                    <td>  130.6939</td>
                    <td> 103.30</td>
                    <td> 0.40</td>
                  </tr>
                </tbody>
              </table>
            </div>
            <div class="panel-footer">Table of results for master/slave.</div>
          </div>

        <h4>Using MapReduce to generate even more solutions</h4>

        <p>The ultimate goal was to get high fitness scores from the non-linear heuristic. To be even more exhaustive, we took all feasible solutions from the parallelized ILP. These solutions were fed into a MapReduce program which perturbed each arrangement by switching every possible pair of students and then checking the non-linear fitness of the perturbed solution. We wrote this MapReduce job in Python using the <code><a href="http://pythonhosted.org/mrjob/">mrjob</a></code> package, and ran it on 10 <code>m1.large</code> Amazon AWS EC2 instances.</p>

        <p>While these perturbed solutions were not guaranteed to obey the hard constraints &mdash; many did not &mdash; that did not necessarily mean they would all have worse fitness scores (although it was likely since the heuristic heavily penalizes violated constraints). We wanted to get an idea of how sparse higher fitness scores would be for solutions that were "near" those generated by the ILP.</p>

        <div class="panel panel-default">
          <div class="panel-body">
            <img src="img/mapreduce.png" width="600" />
          </div>
          <div class="panel-footer">The MapReduce solution perturbation scheme.</div>
        </div>

        <p>As expected, it turned out that very few of the generated perturbations had fitness scores that were higher, and of those that did the difference was small. Of over 6,000,000 perturbed solutions evaluated:

        <ol>
          <li>Only 7,718 did not have worse fitness scores than the original solution.</li>
          <li>Of those 7,718, only 772 scored higher than the original.</li>
          <li>Of those 772 that scored higher, the largest observed difference between the original fitness score and the perturbed solution score was only 500 points &mdash; not a significant difference for scores that are routinely in the neighborhood of 17,000. In fact, most differences were fewer than 200 points.</li>
        </ol>

        </p>

        <div class="panel panel-default">
          <div class="panel-body">
            <img src="img/result-mapreduce.png" width="600" />
          </div>
          <div class="panel-footer">Histogram of fitness differences for the few perturbed solutions that were not worse.</div>
        </div>

        <h4>Conclusion</h4>

        <p>We achieved a gratifying speed increase &mdash; over 100x &mdash; by parallelizing this problem, going from the order of almost 4 hours to about 2 minutes using 256 processors. Even the most naive implementation represented a massive improvement simply by moving from serial to parallel.</p>

        <div class="panel panel-default">
        <div class="panel-body">
            <img src="img/result-all4.png" width="600" />
        </div>
        <div class="panel-footer">Comparison of results from different methods.</div>
        </div>
        <p>With large numbers of processors at your disposal, the master/slave model with load rebalancing is the most efficient of our methods. Were the domain larger, you could even potentially implement a model with two or more masters and many slaves that might run more efficiently with very large numbers of processors, but for a domain of our problems size, it did not make sense to implement this method. In practice, if this model is used for actual class placements, it will probably be run on a laptop or desktop computer with 2-8 cores. In that case, the most sensical implementation to use would be the distributed load balancing one, since that is more efficient with smaller numbers of processes, even though it does not scale up well.</p>
        <p>When using this model for actual class placements, the school is likely to want the results from the top 4 or 5 placements as scored by the non-linlear evaulation function. Integrating the code with the software that the school currently uses to place students was outside the scope of this project. However, due to the significant speedups from utilizing multiple cores, especially for low numbers of cores where efficiency is very high, it is likely that this parts of this project will be implemented into the school's software in the future.</p>
          
        </div>
      </div>

      <div class="footer">
        <p>D. Newman, I. Slavitt, 2013</p>
      </div>

    </div> <!-- /container -->


    <!-- Bootstrap core JavaScript
    ================================================== -->
    <!-- Placed at the end of the document so the pages load faster -->
    <!-- jQuery (necessary for Bootstrap's JavaScript plugins) -->
    <script src="https://code.jquery.com/jquery.js"></script>
    <!-- Include all compiled plugins (below), or include individual files as needed -->
    <script src="//netdna.bootstrapcdn.com/bootstrap/3.0.2/js/bootstrap.min.js"></script>
  </body>
</html>
